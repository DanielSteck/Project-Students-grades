---
title: "Project_student_grades"
subtitle: "Programming languages for Data Science"
author: "Daniel Steck"
date: "04.07.2023"
format: 
  html:
    code-fold: true
    code-tools: true
output:
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

# Academic Honesty Statement

I, Daniel Steck, hereby state that I have not communicated with or gained information in any way from my classmates or anyone other than the Professor during this exam, and that all work is my own.

# Introduction

This project was created within the module "Programming languages for Data Science.This project will follow the data science life-cycle, which consists of four main topics:

-   Plan

-   Data

-   Model

-   Deployment

This project focuses on using all techniques which were learned during the lecture. Moreover it has the target to learn further topics during the project and to implement them (e.g. Deployment via ventiver or shiny).

![Data Science Lifecycle](Images/Data_Science_lifecycle.PNG)

The whole documentation of this project can be also found on Github: [Github Repo: Student Grades](https://github.com/DanielSteck/Project-Students-grades)

# Plan

Defore a project can be started the planning needs to be made first. Without the planning for the project the result often is not what was requested at the beginning. Therefore it is necessary to define what shall be made in the project and what are the targets of this project. Moreover, an usecase is needed as the project shall bring a benefit for the company or the results and models can be sold to others.

To sum is up this chapter is basis for the project and defines the usecase, the problem which shall be solved in this project and the targets which shall be achieved in order to determine at the end if the project was successful or not.

Note: The description in this chapter is without sources for the statements as the target was to describe a fitting plan chapter for this project. Some statements wont be applicable, but in the following they will be used as an assumption for the project.

## Identify use case

We are a new start-up which is solving issues in companies with the latest data science technology. As we are not know yet in the market our costumers are a bit skeptical regarding our skills. Due to that we got the job to show our skills within this project. For this our costumer provided this dataset from kaggle:

-   [Predict the performance of students](https://www.kaggle.com/datasets/rkiattisak/student-performance-in-mathematics?resource=download)

This dataset contains 1000 observations of students and their performance on their exams. Moreover, several further information for each student is given.

The target is to predict the performance of students based on demographic and other factors. In addition to that several research questions shall be answered (e.g which impact has the level of education of the parents on their exam score). In order to prove our skills to the costumer the following topics shall be included within that project:

-   SQL queries

-   R queries

-   Data visualization and the impact on the target variable

-   Data Preprocessing

-   Train different types of machine learning models

-   Determine the best model and deploy it

## Frame the problem

In the past it was noticed that the gap between students with very good marks and the one with rÂ´quite bad ones is getting bigger and bigger, but it is not known yet why this keeps happening. This also leads to a gap when the students finish their education and they are going to work. More and more people are not having the required qualification for their jobs in the future as they are not able to finish their education due to bad marks. This needs to be stopped and changes in the system have to be made in order to improve this and to keep the country ready for the developments in the next decades.

As our costumer is responsible for the education system they are very curious of the results from this project. Based on what can be learned in the queries and visualizations as well as in the models. They plan to make changes in their education system. For example a special assistance for students who are not performing that good. It depends on the results of this project to identify possible target groups who need more support in order to improve their skills and performance in exams. Therefore, it is important to learn which groups have disadvantages in their education and why. Only with this information it is possible to support them. As more and more students with a good mark are finishing their education the qualification in the labour market will increase and this will improve the economic power of our country.

The model shall support on this task. As it will be possible to play around with the characteristics of the students and their background. The model can be the basis for the decisions for the education system.

## Identify variables

The provided dataset contains eight different variables and 1000 observations. Each observation represents one student (data was anonymized) with his/her score in exams and their characteristics and their background.

Variables in the dataset:

-   **Gender:** Gender of the student. This variable has two levels in this dataset. The available data is split either in male or female.

-   **Race/ethnicity:** This columns describe the racial or ethnic background of the students (e.g. Asian, African-American, Hispanic, etc.). Note: This column was transferred in groups without information which group displays which ethnicity. This was done to get the result in this project only on the data itself and to avoid the influence of prejudges and stereotypes.

-   **Parental level of education:** What is the highest level of education attained from the students parents.There is no differentiation between the parents and only the highest level of education is listed here. The level of education from the other parent is ignored.

- **Lunch:** What type of lunch do the students receive: Either the standard type of lunch or a free/reduced option. The free/reduced option is probably only available for students who have a family which is kind of poor.

- **Test preparation course:** Did the students complete a test preparation course or not?

- **Math score:** Score which was achieved by the student in a standardized mathematics test.

- **Reading score:** Score which was achieved by the student in a standardized reading test.

- **Writing score:** Score which was achieved by the student in a standardized writing test.

## Define metrics

# Data

Within this chapter the data will be ingested and analyzed. Furthermore the data is also added into a SQL database and will be used for making queries.

## SQL data analysis

Before the steps of the data science lifecycle are made, some SQL queries will be made on the data, in order to answer questions from our costumers.

First the DBI library is loaded and afterwards the data is added into the database.

```{r import_SQL_librarys}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, collapse = TRUE) 
library (DBI)
library(tidyverse)

```

```{r database_connection}

# Connection to database 
con <- dbConnect(RSQLite::SQLite(), ":memory:")

path_sql <- "https://raw.githubusercontent.com/DanielSteck/Project-Students-grades/main/exams.csv"

# Write data "gapminder" into database
dbWriteTable(con, "exams_sql", read_csv(path_sql, show_col_types = FALSE))

#show table in database

dbListTables(con)
```

### First look on the data

```{sql connection=con}

SELECT *
FROM exams_sql;

```

The dataset contains 1000 observation with eight variables. Three out of these 8 variables are the students performance on different exams.

Show the first 10 observations:

```{sql connection=con}

SELECT *
FROM exams_sql
LIMIT 10;
```

It can be seen that the columns should be renamed as there are e.g. spaces included:

```{sql connection=con}

ALTER TABLE exams_sql
RENAME COLUMN "race/ethnicity" TO ethnicity

```

```{sql connection=con}
ALTER TABLE exams_sql
RENAME COLUMN "parental level of education" TO parent_education

```

```{sql connection=con}
ALTER TABLE exams_sql
RENAME COLUMN "test preparation course" TO prep_course
```

```{sql connection=con}

ALTER TABLE exams_sql
RENAME COLUMN "math score" TO math_score
```

```{sql connection=con}
ALTER TABLE exams_sql
RENAME COLUMN "reading score" TO reading_score
```

```{sql connection=con}
ALTER TABLE exams_sql
RENAME COLUMN "writing score" TO writing_score;
```

Check the renamed columns

```{sql connection=con}
SELECT *
FROM exams_sql
```

Lets have a look on the gender and their performance in math.

```{sql connection=con}

SELECT gender, "math_score"
FROM exams_sql;
```

### Data Exploration

How many different genders are in the dataset?

```{sql connection=con}

SELECT DISTINCT gender
FROM exams_sql;

```

How many different ethnicities are in the dataset?

```{sql connection=con}

SELECT DISTINCT ethnicity
FROM exams_sql;
```

How many different forms of parent_education are in the dataset?

```{sql connection=con}

SELECT DISTINCT parent_education
FROM exams_sql;
```

How many different lunch types are in the dataset?

```{sql connection=con}

SELECT DISTINCT lunch
FROM exams_sql;
```

How many different types of preparation courses are in the dataset?

```{sql connection=con}

SELECT DISTINCT prep_course
FROM exams_sql;

```

How many observations are in the data set?

```{sql connection=con}

SELECT 
  COUNT(*) as rows
FROM exams_sql;
```

How many observations are available for each parent_education

```{sql connection=con}

SELECT parent_education,
  COUNT(parent_education) AS amount_of_observations
FROM exams_sql
GROUP BY parent_education
ORDER BY amount_of_observations DESC;
```

How many students have a math score above 80?

```{sql connection=con}

SELECT 
  COUNT(*) as students_above_math_80
FROM exams_sql
WHERE math_score > 80;
```

Have a look at the obervations of students with more than 90 in reading score

```{sql connection=con}

SELECT *
FROM exams_sql
WHERE reading_score >90;

```

Check the obervations of all females with more than 90 in writing score. The result shall be ordered by writing score (decreasing)

```{sql connection=con}

SELECT*
FROM exams_sql
WHERE gender ="female" AND writing_score>90
ORDER BY writing_score DESC;

```

Check all observations of students with more than 90% in one of the exams and made NO prep_course

```{sql connection=con}
SELECT *
FROM exams_sql
WHERE prep_course = "none" AND (math_score>90 OR writing_score>90 OR reading_score>90);

```

What are the average exam scores of the different ethnicies?

```{sql connection=con}
SELECT ethnicity,
  AVG(math_score) as avg_math,
  AVG(reading_score) as avg_read,
  AVG(writing_score) as avg_write
FROM exams_sql
GROUP BY ethnicity;
```

What are the best exam scores?

```{sql connection=con}

SELECT 
  MAX(math_score) as max_math,
  MAX(reading_score) as max_read,
  MAX(writing_score) as max_write
FROM exams_sql;
```

What are the worst exam scores?

```{sql connection=con}

SELECT 
  MIN(math_score) as min_math,
  MIN(reading_score) as min_read,
  MIN(writing_score) as min_write
FROM exams_sql;
```

What are the average exam scores for the genders?

```{sql connection=con}

SELECT gender,
  (AVG(math_score) + AVG(reading_score) + AVG(writing_score)) /3 as avg_exams_score
FROM exams_sql
GROUP BY gender;
```

Show all obervations which are parents_education "high_school" and have an average score in exams between 80 and 90. Result shall be ordered descending by avg_exam_score

```{sql connection=con}

SELECT gender, ethnicity, parent_education, lunch, prep_course,
  (math_score + reading_score + writing_score) /3 as avg_exams_score
FROM exams_sql
WHERE parent_education = "high school" AND avg_exams_score BETWEEN 80 AND 90;
```

Show all male ethnicities which have an average_exams score less than 70. Order the avg_exams_score ascending.

```{sql connection=con}

SELECT ethnicity,
  (AVG(math_score) + AVG(reading_score) + AVG(writing_score)) /3 as avg_exams_score
FROM exams_sql
WHERE gender="male"
GROUP BY ethnicity
HAVING avg_exams_score < 70
ORDER BY avg_exams_score;
  

```

## Data ingestion

After the SQL queries were successfully made the next step is to continue with the steps in R. All following steps will be made within R.

### Data Import

First of all the data will be imported from a github repo. The initial data set can be found on Kaggle: [Kaggle: Student performance prediction](https://www.kaggle.com/datasets/rkiattisak/student-performance-in-mathematics?resource=download)

In order to have the same data the whole time this dataset was downloaded and added to my github repo. in case that the kaggle dataset might be updated.

```{r read_csv}

library(tidyverse)

path <- "https://raw.githubusercontent.com/DanielSteck/Project-Students-grades/main/exams.csv"

exams <- read_csv(path, show_col_types = FALSE)

```

### Clean Data

Lets have a look on the first rows to get an expression of the data.

```{r first_rows}

exams %>%
  slice(1:5)
  

```

Before further steps can be made, the columns names need to be corrected, as they are including spaces and special characters. To remove those, the janitor package will be used.

```{r janitor}
library(janitor)
exams <- exams %>%
  clean_names()
```

Beside the cleaning of the names, some variables shall be renamed in order to shorten the length.

```{r rename_columns}
exams <- rename(exams, ethnic_group = race_ethnicity)
exams <- rename(exams, parent_education = parental_level_of_education)
exams <- rename(exams, test_prep = test_preparation_course)
```

The available data consist of `r nrow(exams)` rows. The available features are `r names(exams)`.

### Missing data

Lets have a look on the description of the data to check the formats.

```{r glimpse}
glimpse (exams)
```

Is there any missing data? lets check it:

```{r missing_values_graphic}
library(visdat)
vis_dat(exams)
```

Alternative method for missing data:

```{r missing_values_num}
is.na(exams) %>% colSums()
```

There are no missing numbers in the dataset. Three variables (scores in the exam) are numeric, the other variables are characters.

### Format data

Lets have a look on the character columns and check if they should be formatted as factor.

```{r levels_gender}
exams %>%
  count(gender,
        sort = TRUE)
```

```{r levels_ethnic_group}
exams %>%
  count(ethnic_group,
        sort = TRUE)
```

```{r levels_parent_education}
exams %>%
  count(parent_education,
        sort = TRUE)
```

```{r levels_lunch}
exams %>%
  count(lunch,
        sort = TRUE)
```

```{r levels_test_prep}
exams %>%
  count(test_prep,
        sort = TRUE)
```

All character variables are having only a small amount of different levels. Therefore all character variables shall be transferred as factor.

```{r char_to_factor}
exams <- 
  exams %>% 
  mutate(across(where(is.character), as.factor))
```

### Create new variables

Target of this project is to predict the students performance on exams. Therefore an additional column shall be added. This column shall include the average exam score of the students.

```{r new_column_avg_score}
exams <- exams%>%
  mutate (avg_score = (math_score + reading_score + writing_score)/3)
```

As all other variables are not numeric, no further new variables can be created.

### Data overview

After successfully cleaning, formatting and checking the data, lets have an overview of the data with the package *skimr*:

```{r skimr}
library(skimr)
skim(exams)
```

The available data set consists of 1000 rows with in total 9 columns. Four columns are numeric while five are a factor.

Three factor variables have only two levels, while the other two have five and six levels. The distribution of the gender column in the two genders is quite balanced. The columns lunch and test_prep are split in 2:1 in their variables.

The distribution of the numeric variables is very similar. in general the math_score has the lowest values compared to the other three scores, but the difference is not that big. The statistics for p25, p50, p75, p100 as well as the histograms are looking very similar and the differences are only minor. The reading score has the best results in the exam.

### Define features and outcome variable

In advance of the data splitting the definition of the target variable and the feature variables shall be made.

The performance in the exams shall be predicted, for this the average performance is sufficient. As the target is to predict the performance, the variables for math, reading and writing score must not be included into the feature list.

It would be also possible to make a use case which predicts e.g. the performance in math and also includes the students performance in the other exams. In this case only the general performance shall be predicted to see which generic attributes have a influence on students performance. Also exams are most time in a thigh schedule, which means that there wont be a lot of time between exams. Therefore it does not make sense to include also the performance in other exams to predict the performance in e.g. math. Moreover it takes some time until the results are available. So probably the students already had their exam before they receive the first results. Therefore the target in this project is the prediction of an average exam performance without including the performance in the other exams.

Nevertheless in the data exploration these other exam scores will be also included, but they wont go into the model.

```{r outcome_and_features}
y_label <- 'avg_score'
features <- c('gender', 'ethnic_group', 'parent_education', 'lunch', 'test_prep')
X <- exams %>%
  select(all_of(features))
y <- exams %>%
  select(all_of(y_label))
```

Our data is now prepared for the data splitting and for building the pipelines.

## Data splitting

Before the data exploration can be started, the data shall be split into a training and test set.

To ensure that the training and test set is representative of the various categories of `avg_score` in the whole dataset, lets have a look on the histogram

```{r bins_target_variable}
exams %>%
  ggplot(aes(avg_score)) + 
  geom_histogram (bins=6) #tested several, 6 makes most sense 

```

The split data shall included equally data from each of the bins. In order the have the same results for each run, a fix seed is set.

Target is to have 80% of the data for training and 20% for testing.

```{r train_test_split}
library(rsample)

set.seed(50) #for reproducability

# split in train and test data
data_split <- initial_split (exams,
                             prop =4/5,
                             strat = avg_score,
                             breaks = 6)

# Create train dataframe
train_data <- training(data_split)

#create test data set
test_data <- testing(data_split)
```

Cross Validation was removed here, as it makes more sense to add it after the data was prepared within the recipe.

## Analyze data

This chapter had the target to understand the training data and to see which variables have an influence on the outcome variable **avg_score**.

### Data exploration set

First step is to create a copy of the training data which will be used for the exploratory data analysis.

```{r exploration_dataframe}
df_explo <- train_data
```

### R queries

First of all the data is explored with R queries and afterwards the visualization part is made.

Lets have a look on the statistics of the target variable *avg_score*\*:

```{r avg_score_statistics}
df_explo %>%
  summarise(
    min_score = min(avg_score,na.rm =TRUE) # na dependency not needed as no missing values, but it might be that data changes with an updated dataset
    ,
    mean_score = mean(avg_score,na.rm =TRUE),
    med_score = median(avg_score,na.rm =TRUE),
    std_score = sd(avg_score,na.rm =TRUE),
    iqr_score = IQR(avg_score,na.rm =TRUE),
    q1 =quantile(avg_score, probs =0.25,na.rm =TRUE),
    q3 = quantile(avg_score, probs = 0.75,na.rm =TRUE),
    max_score = max(avg_score,na.rm =TRUE)
    
  )
```

The worst exam result is `20` while `100` is the best one. Moreover the mean and median values are almost similar and there is only a slight difference. This means that there are no big outliers included here (which is also easy to explain as 100 is the best possible performance in an exam). The IQR score of 20 tells us, that the difference between the 75th and 25th percentile is only having a difference of 20. So 50% of the data is within a difference of 20.

25% of the students have an exam score of less than 59,3 and 25& of the students are above 79,3. 50% of the students are between 59,3 and 79,3 (the IQR value can be seen here)

Moreover it is interesting how the median, mean and iqr, q1, q3 values are distributed within the categorical variables.

Lets have a look on the gender variable and sort the results descending.

```{r avg_score_gender}
df_explo %>%
  group_by(gender) %>%
  summarise(
    mean_score = mean(avg_score,na.rm =TRUE),
    med_score = median(avg_score,na.rm =TRUE),
    iqr_score = IQR(avg_score,na.rm =TRUE),
    q1 =quantile(avg_score, probs =0.25,na.rm =TRUE),
    q3 = quantile(avg_score, probs = 0.75,na.rm =TRUE)) %>% 
      
arrange(desc(med_score))
```

The performance of female students is better than the performance of male students. The mean score for females is 3.3 higher and the median score is 4.0 higher.

Lets have a look on the ethic_group variable and sort the results descending.

```{r avg_score_ethic_group}
df_explo %>%
  group_by(ethnic_group) %>%
  summarise(
    mean_score = mean(avg_score,na.rm =TRUE),
    med_score = median(avg_score,na.rm =TRUE),
    iqr_score = IQR(avg_score,na.rm =TRUE),
    q1 =quantile(avg_score, probs =0.25,na.rm =TRUE),
    q3 = quantile(avg_score, probs = 0.75,na.rm =TRUE),
    ) %>%
arrange(desc(med_score))
```

The `group E` has the best performance in the exams. There is a huge gap compared to `group D`. Moreover there is almost no difference in mean and median between the other three groups. It might make sense to combine there groups into one group. But lets analyse this first in a visualization, because the IQR score is spreading a lot between these groups. For example the difference in the q1 is much higher than in the mean and median.

Lets have a look on the variable of parents education and sort the results descending.

```{r avg_score_parent_education}
df_explo %>%
  group_by(parent_education) %>%
  summarise(
    mean_score = mean(avg_score,na.rm =TRUE),
    med_score = median(avg_score,na.rm =TRUE),
    iqr_score = IQR(avg_score,na.rm =TRUE),
    q1 =quantile(avg_score, probs =0.25,na.rm =TRUE),
    q3 = quantile(avg_score, probs = 0.75,na.rm =TRUE),
    ) %>%
arrange(desc(med_score))
```

It can be seen that a higher parent education is not directly leading to a better exam score. The best students are within the `associate's degree` which is a lower education than the `master's degree` and `bachelor's degree`.

Based on the given levels, this would be the ascending order of parent_education (based on these links):

-   [Associates, Bachelors and Masters](https://thebestschools.org/degrees/college-degree-levels/#:~:text=A%20doctorate%20is%20the%20highest,research%20requirements%2C%20and%20a%20dissertation.)

-   [High School and College](https://usahello.org/education/children/grade-levels/#gref)

-   [College vs Associates degree](http://www.differencebetween.net/miscellaneous/difference-between-diploma-and-associate-degree/#:~:text=A%20student%20may%20take%20one,stone%20to%20a%20higher%20education.)

List of education level in a ascending order:

1.  Some High School/High School --\> I did not find any results for this in a research. Probably High School might be a bit better than some High school
2.  Some College
3.  Associates degree
4.  Bachelor degree
5.  Masters degree

This order of parental education is not represented in the performance of students performance.

The worst exam results are for the parental education of some High School and High School.

Lets have a look on the lunch variable and sort the results descending.

```{r avg_score_lunch}
df_explo %>%
  group_by(lunch) %>%
  summarise(
    mean_score = mean(avg_score,na.rm =TRUE),
    med_score = median(avg_score,na.rm =TRUE),
    iqr_score = IQR(avg_score,na.rm =TRUE),
    q1 =quantile(avg_score, probs =0.25,na.rm =TRUE),
    q3 = quantile(avg_score, probs = 0.75,na.rm =TRUE),
    ) %>%
arrange(desc(med_score))
```

There is a huge difference between the students performance which have a standard meal and the ones which have a free/reduced meal. Probably the students who are getting a free/reduced meal are from a porerer family which might have less time to spend with their kids or maybe the kids education is not the highest priority. As the family might have financial issues it could be also possible that the students need to work in order to have enough money for the family. Therefore it might be possible that their study time is needed for working and this might lead to worse results.

Lets have a look on the test preparation variable and sort the results descending.

```{r avg_score_test_prep}
df_explo %>%
  group_by(test_prep) %>%
  summarise(
    mean_score = mean(avg_score,na.rm =TRUE),
    med_score = median(avg_score,na.rm =TRUE),
    iqr_score = IQR(avg_score,na.rm =TRUE),
    q1 =quantile(avg_score, probs =0.25,na.rm =TRUE),
    q3 = quantile(avg_score, probs = 0.75,na.rm =TRUE),
    ) %>%
arrange(desc(med_score))
```

Students who made a test preparation course have much better results than students who didn't. But the difference is much smaller than in the lunch variable.

### Visualizations

In this chapter plots are created in order to learn from the data and find correlations between the variables.

In order to select the correct visualizations the visualizations are based on [From Data to Viz](https://www.data-to-viz.com)

First of all lets start with the plot of the target variable. How is it distributed in the value range?

```{r dense_plot_avg_score}
library (ggplot2)
ggplot(data = df_explo, mapping = aes(x = avg_score)) +
  geom_density(fill="lightblue") +
  labs(title = "Students performance in exams", 
       subtitle= "Average performance of math, writing and reading exam",
       x = "Average score",
       y = "Density") +
  theme_minimal()+
  theme (text = element_text(size=8),
         legend.key.size = unit(5, "mm"))


```

The results are quite similar compared to what was learned in the R queries. Highest density of the average_score is at the median score. There are only a small amount of scores which are below 40.

Now lets have a look on our features. For this use case we only got categorical features in our data set. Therefore lets have a look on their levels and the uniqueness.

```{r cat_features_bar_plot}
for (i in features){
  
  p <- ggplot(df_explo, aes_string(x=i)) +
  geom_bar(fill="lightblue")+
    labs(title = "Distibution of levels in the varaibles", 
       y= "count") +
  theme_minimal()+
  theme (text = element_text(size=8),
         legend.key.size = unit(5, "mm"))
  
  plot(p)
  }
```

It can be seen that there the variable **gender** is equally distributed on male and female.

The **ethnic group** has the most occurrences within group C, while group A and group E have a smaller occur ency. Especially group A has only about 50 occurrences.

Regarding the **parents education**- this feature has six levels which are not equally distributed. Four levels has a almost similar occurrence, while the level Bachelors and Masters degree have the lowest occurrences.

The **lunch** type has two levels which are not equally distributed, about 66% occurrences are the standard lunch types, while 33% are the free/reduced type of lunch. This is actually quite a high number of reduced/free lunch meals.

The amount of students who performed a **test preparation** course is about 35% while 65% of students did not make any test preparation.

Lets have a look on the impact of these features and their levels on the target variable **average score**

For this project the visualizations of categorical and the target variable will be made with box plots, density plots, violin plots and histograms in order to show the different possibilities in R. Therefore the variables will be displayed in different graphical visualizations. In general box plots would be a very good way to visualize these results, but for demonstration purposes different visualizations are made.

**Impact Gender on Avg_score**

```{r avg_score_gender_histogram}

ggplot(data = df_explo, mapping = aes(x = avg_score, fill=gender)) +
  geom_histogram(bins=20) +
  labs(title = "Students average performance in exams", 
       subtitle= "By gender",
       x = "Average score in exams",
       y = "count") +
  theme_minimal()+
  theme (text = element_text(size=8),
         legend.key.size = unit(5, "mm"))
```

Actually in this plot it is not that easy to see the differences between those two genders. So therefore lets try it with a facet wrap.

```{r avg_score_gender_histogram2}

ggplot(data = df_explo, mapping = aes(x = avg_score, fill=gender)) +
  geom_histogram(bins=20) +
  facet_wrap(~gender, ncol=1)+
  labs(title = "Students average performance in exams", 
       subtitle= "By gender",
       x = "Average score in exams",
       y = "count") +
  theme_minimal()+
  theme (text = element_text(size=8),
         legend.key.size = unit(5, "mm"))
```

Now it can be seen that female are performing much better than the males. There are a lot of test results above 80, while the males have a peak at above 75. But it is interesting that the are much more males who achieve 100 points in the exam compared to the women.

**Impact ethnic_group on Avg_score**

The different ethnic groups and their impact on the avg_score shall be displayed within a boxplot.

```{r avg_score_ethnic_group_box_plot}
library(forcats)
ggplot(df_explo, aes(x = fct_reorder(
  ethnic_group, avg_score, .fun = median, .desc=TRUE), y=avg_score)) +
  geom_boxplot(fill="lightblue") +
  labs(title = "Students average performance in exams", 
       subtitle= "By ethnic group",
       x = "Ethnic group",
       y = "Average score in exams") +
  theme_minimal()+
  theme (text = element_text(size=8),
         legend.key.size = unit(5, "mm"))
```

Actually the results from the R queries can be confirmed. Best results are in `group E` and the groups `A`, `B` and `C` have almost the same values. Even though there are some outliers in `group C`. The mean for these three groups is almost the same, but it can be seen that `group B` has different whiskers, here the IQR range is spread the most.

**Impact parent_education on Avg_score**

This variable in their influence on the average score shall be visualized in a violin plot.

```{r avg_score_parent_edu_violin_plot}

ggplot(df_explo, aes(x = fct_reorder(
  parent_education, avg_score, .fun = median, .desc=TRUE), y=avg_score)) +
  geom_violin(fill="lightblue") +
  labs(title = "Students average performance in exams", 
       subtitle= "By parents education",
       x = "Parent Education",
       y = "Average score in exams") +
  theme_minimal()+
  theme (text = element_text(size=8),
         legend.key.size = unit(5, "mm"),
         axis.text.x = element_text(angle = 90))
```

Quite similar results as in the R queries. It can be seen that the `associates degree` as parent education leads to the best median score. But it can be seen that there are also a lot of students in this group which have results of less than 40. Only in two other groups the results are worse. In five out of six parent degrees the most students are concentrated nearby the median score. But the `high_school` degree has a peak of at the average score of 63 and as well another, smaller peak at the peak of 80. Moreover on this group there are the students with the worst results. Furthermore the group of `some college` and `bachelors degree` have a very similar violin plot and there are only minor differences between those two groups.

**Impact lunch on Avg_score**

The impact of the lunch type shall be displayed within a density plot. Moreover, this plot shall be extended by the **Impact test preparation course on Avg_score**.

```{r lunch_test_prep_on_avg_score_dense}

ggplot(df_explo, mapping = aes (x=avg_score, fill=lunch)) +
  geom_density(alpha=0.7) + 
  scale_fill_manual("Lunch", values = c("#339999", "#FFFF66"))+
  facet_wrap(~test_prep, ncol=1)+
  labs(title = "Students average performance in exams", 
       subtitle= "By type of lunch and if a test preperation course was made",
       x = "Average score in exams",
       y = "density") +
  theme_minimal()+
  theme (text = element_text(size=8),
         legend.key.size = unit(5, "mm"))

```

It can be clearly seen that the students who completed the test preparation course have a much better performance than those who did not. Also the students who have a standard meal are performing better than those who are having only a free or reduced meal. If both features are combined it can be clearly seen that students who made the test preparation course AND have a standard meal are performing much better than those students who did not make a test preparation course and who only get a free/reduced meal. These two features have a big impact on the target variable.

### Visualizations (incl. perform results)

This chapter is used to show the possibilities of data visualizations if we could use numeric variables. Therefore plots correlations will be made with those variables - even though they must not be included into the later models.

In order to select the correct visualizations the visualizations are based on [From Data to Viz](https://www.data-to-viz.com)

Lets start with the students performance in math compared to their performance in the average exam score.

```{r math_avg_score_scatter}
ggplot(df_explo, aes(math_score, avg_score))+
  geom_point(color="lightblue")+
  labs(title = "Students average performance in exams", 
       subtitle= "Average performance and math performance",
       x = "Average score in maths",
       y = "Average score in exams") +
  theme_minimal()+
  theme (text = element_text(size=8),
         legend.key.size = unit(5, "mm"))
```

It can be clearly seen that there is a linear relationship between the math performance and the average performance (which makes sense as the average performance is based on the math performance, but this is made for demonstration purposes). The correlation between those variables is very strong.

Lets add two further variables to this plot. Lets add the type of lunch and the test preparation as well.Moreover it shall be wrapped by the ethnic group.

```{r impact_avg_score_scatter}
ggplot(df_explo, aes(math_score, avg_score, color=lunch, shape=test_prep))+
  geom_point()+
  facet_wrap(~ethnic_group)+
  labs(title = "Students average performance in exams", 
       subtitle= "By lunch type, ethnic_group and if test preparation was made",
       x = "Average score in maths",
       y = "Average score in exams") +
  theme_minimal()+
  theme (text = element_text(size=8),
         legend.key.size = unit(5, "mm"))
```

It can be seen that the performance in group E and D is the best, as well as that students with standard lunch and test preparation course are performing better.

Now lets have a look on the correlations between our three exam scores and the average exam score.

```{r correlations}
df_explo %>% 
  select(where(is.numeric)) %>% # only select numerical data
  vis_cor(cor_method = "spearman", na_action = "pairwise.complete.obs")
```

It can be seen that all variables have a huge correlation with each other. Students which are performing good in one exam are likely to perform goog in all exams. But it can be also seen that the correlation between writing and reading is much higher than the correlation to math and the other exams.

Lets have a look on the numbers:

```{r correlation_numbers}
library(corrr)

cor_res <- 
  df_explo %>%
  select(where(is.numeric)) %>% 
  correlate(method = "spearman", use = "pairwise.complete.obs") 

cor_res %>% 
  select(term, avg_score) %>% 
  filter(!is.na(avg_score)) %>% #  dependent variable 
  arrange(avg_score) %>% # sorting
  fashion() # print 
```

Math score has the lowest impact on the average score, while writing and reading scores have the highest. As learned in the correlation matrix before: Writing and Reading are correlating with each other. Therefore this result was expectable.

Now lets analyze the numeric variables in more detail. Moreover the categorical variable "lunch" shall be added as well.

```{r visual_inspection}
library(GGally)
df_explo %>% 
  select(avg_score, lunch, 
         math_score, reading_score, writing_score) %>% 
  ggscatmat(color="lunch", corMethod = "spearman",
            alpha=0.4)
```

As the numeric variables are already so good in predicting the avg_score, the impact of the lunch type is no longer seen. In the analysis before it was clearly seen that this variable has a big impact on the avg_score.

## Anomaly detection

Within this chapter the data is checked for outliers and missing data. This is done in the original dataframe. This is done so that it can be learned which steps need to be made in the pipeline definitions. No changes are made with the data, it is only checked how to deal with detected issues.

### Missing values

This was already performed in the Data Ingestion chapter, nevertheless it is performed again in this chapter.

```{r missing_values_data}
is.na(exams) %>%
  colSums()
```

There are no missing data/rows in the available data.

### Outlier and novelty detection

Lets check our data for outliers - as it was already made during the Data analysis. For the categorical data there are no outliers included, as they are only spread through a limited set of levels. Moreover there is no level included which is underrepresented. Therefore the check for outliers is only made in the target variable.

Lets create a plot of the target variable in box plots with all categorical data.

```{r outlier_detection}
for (i in features){
    
    p <- ggplot(df_explo, aes_string(x=i, y=y_label)) +
      geom_boxplot(fill="steelblue")
      
    plot(p)
  }
```

It can be seen that there are certain outliers included in the available data. These outliers are certain students who performed very bad in the exams. The training data contains outliers, these are defined as observations which are far from the others. Outlier detection estimators thus try to fit the regions where the training data is the most concentrated, ignoring the deviant observations.

These outliers make it hard for the later model to predict the result correctly. The learning from this chapter shall be included in the later model creation.

## Feature engineering

All topics which were learned/discovered within the earlier steps shall be used now for the data preprocessing and feature engineering. This will be made with data pipelines. Those are generated with the tidymodels package *recipes* and *workflows*. The feature selection part wont be included here, as the features are selected within the modeling part. So the recipe will include all available features for a base model. In the model chapter the Feature Selection will be made with the available packages and tools in R.

### Feature transformation

Target of feature transformations - for this project and maybe new data in the future is to get red of:

-   missing values
-   outliers
-   categorical variables --\> one hot encoding

Other datasets would need further steps which do not apply in this project due to the data structure. Those other steps could be:

-   get data on the same scale

-   standardize the data

-   handle skewed distribution of data

-   take care if the data might be censored at certain limits (e.g. at the low and or high of the data)

**Missing values:** As we do not have any missing data in the data set, this step could be skipped. But in case that there might be newer data in the future which could have missing data, this should be added to the pipeline. There are three ways to handle missing data: Delete the observation with missing data, remove the whole attribute or impute it with some other values, e.g. the mean, median, etc. For this project missing data shall be dropped within the pipeline.

**Outliers:** Some outliers are included in the data set. For handling those the target is to test the experimental 3-rd party package from [*tidy_outlier*](https://github.com/brunocarlin/tidy.outliers)

**One-Hot_encoding:** Our available categorical data needs to be converted to numbers as algorithms prefer to work with numbers. For this the tidy models [step_dummy](https://recipes.tidymodels.org/articles/Dummies.html) function will be used.

### Feature extraction

Some features might include relevant information which could be redundant. That would be there wont be any benefit in adding both features in the model, if the target variable can be determined with one of those.

Therefore it makes sense to remove features which have large correlations with each other. But this is more likely at numeric features. In the feature extraction this can be made by using the ratio of two two predictors or with more complex methods like principal component analysis.

This also makes sense to use in case that the number of features is high.

For this usecase this is only partly applicable, as there are only categorical features available.

### Final data pipeline

In the following pipeline all steps for the feature engineering are included:

```{r get_tidy_outliers}
#devtools::install_github("brunocarlin/tidy.outliers") 
library(tidy.outliers)
```

```{r pipeline}

library(recipes)
exam_rec <-
  recipe(avg_score ~ +gender  + ethnic_group + parent_education + lunch + test_prep, data = train_data) %>% #definition of features and target variable
  step_naomit(everything(), skip = TRUE) %>% #remove missing observations
  step_dummy(all_nominal()) %>% #converts factors into numeric binary (0 and 1) variables
  step_outliers_maha(all_numeric(), -all_outcomes()) %>%
  step_outliers_lookout(all_numeric(),-contains(r"(.outliers)"),-all_outcomes()) %>%
  step_outliers_remove(contains(r"(.outliers)")) %>%
  step_corr(all_predictors(), threshold = 0.7, method = "spearman") # remove correlated features (maybe not necessary as only character features as input)
```

Lets have on the current set of variables and roles:

```{r summary_recipe}

summary(exam_rec)

```

Lets check if all preprocessing steps worked as intended:

```{r juice}

prepped_data <- 
  exam_rec %>% # use the recipe object
  prep() %>% # perform the recipe on training data
  juice() # extract only the preprocessed dataframe 

glimpse(prepped_data)

```

The recipe was successful! The character columns were successfully changed to dummy variables, so that the models can work with them. Moreover three observations were removed as they were identified as outliers.

Moreover a validation set shall be created. This shall be used to validate the later models. After best models is used, based on the validation set, the model can be tested with the test data. The amount of breaks is the value which was learned in the Data Visualization chapter.

```{r validation_set}
set.seed(12)

cv_folds <- 
  vfold_cv(train_data,
           v=5,
           strata = y_label,
           breaks = 6)
```

# Model

After the data and features have been preprocessed in a format which is ready for the modeling algorithms, they can be used in the model selection process.

Target for the Model and Deployment phase is to perform all steps in the usual way with tidymodels and afterwards one time by using MLFfow.

## Select algorithm

The target is to predict the *avg_score* of students in exams. This outcome variable is numeric. Therefore a **Regression** model needs to be selected for this usecase. The following page is used to select three different model types and their engines: [Tidymodels: Search parsnip models](https://www.tidymodels.org/find/parsnip/)

The following three models were selected:

-   Boosted Tree (XGBoost)

-   Linear Regression (~~Keras~~ - changed to glmnet as I had issues to get keras and tensorflow running. For this a python installation and connection is needed)

-   Single layer neural network (~~Keras~~ - changed to nnet, see above)

## Model Specification

Regarding the model specification three steps need to be made:

1.  Pick a model type (see above)
2.  Set the engine (see above)
3.  Set the mode regression (as the target is to perform a regression)

The further chapters are based on the documentation of the following pages:

-   [Tidymodels: Bossted Tree](https://parsnip.tidymodels.org/reference/boost_tree.html)

-   [Tidymodels: Linear Regression](https://parsnip.tidymodels.org/reference/linear_reg.html)

-   [Tidymodels: Single layer neural network](https://parsnip.tidymodels.org/reference/mlp.html)

-   [Tidymodels: Tune](https://tune.tidymodels.org)

### Boosted Tree

At first the target is to keep the model simple, that's why the model will be created with standard settings:

```{r spec_xgboost_1}
library(tidymodels)
library(xgboost)
xgb_spec_1 <- 
  boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("regression") 
```

Add the created model into a workflow:

```{r workflow_xboost_1}

xgb_workflow_1 <-
  workflow() %>%
  add_recipe(exam_rec) %>%
  add_model(xgb_spec_1)
```

### Linear Regression

At first the target is to keep the model simple, that's why the model will be created with standard settings:

```{r spec_lin_reg_1}
library(keras)
library(tensorflow)
lin_spec_1 <- 
  linear_reg(penalty = 0.1, mixture = 1) %>% 
  set_engine("glmnet") %>% 
  set_mode("regression") 
```

Add the created model into a workflow:

```{r workflow_lin_reg_1}

lin_reg_workflow_1 <-
  workflow() %>%
  add_recipe(exam_rec) %>%
  add_model(lin_spec_1)
```

### Single Layer Neural Network

At first the target is to keep the model simple, that's why the model will be created with standard settings:

```{r spec_neural_1}

neural_spec_1 <- 
  mlp() %>% 
  set_engine("nnet") %>% 
  set_mode("regression") 
```

Add the created model into a workflow:

```{r workflow_neural_1}

neural_workflow_1 <-
  workflow() %>%
  add_recipe(exam_rec) %>%
  add_model(neural_spec_1)
```

## Model training & evaluation

### Boosted tree

```{r res_xgboost_1}
set.seed(50)

xgb_res_1 <-
  xgb_workflow_1 %>%
  fit_resamples(
    resamples = cv_folds,
                control = control_resamples(save_pred = TRUE)
  )
xgb_res_1 %>% collect_metrics(summarize = TRUE)
```

How are these metrics spreading through the cross_validation:

```{r res_xgboost_cv}
xgb_res_1 %>% collect_metrics(summarize = FALSE)
```

There are some smaller differences between these folds, but overall those differences are not to big.

Lets visualize these metrics into a plot. For this the predictions are plotted on the y-axis and the actual values on the x-axis. If a perfect model would have been created, all points would be on the diagonal line

```{r plot_xgboost_res_1}

assess_xgb_res_1 <- collect_predictions(xgb_res_1)

assess_xgb_res_1 %>% 
  ggplot(aes(x = avg_score, y = .pred)) +   geom_point(alpha = .15) +
  geom_abline(col = "blue") + 
  coord_obs_pred() + 
  ylab("Predicted") +
  xlab("Actual Average Exam Score")



```

If we consider that this is the first created model and that there are only categorical features in the dataset, this result not that bad. Lets see in the next chapter how the Boosted Tree Performance can be increased. Further steps are to adapt the hyperparameters and the features which are added into the model.

### Linear Regression

```{r res_lin_reg_1}
set.seed(50)

lin_reg_res_1 <-
  lin_reg_workflow_1 %>%
  fit_resamples(
    resamples = cv_folds,
                control = control_resamples(save_pred = TRUE)
  )

lin_reg_res_1 %>% collect_metrics(summarize = TRUE)
```

How are these metrics spreading through the cross_validation:

```{r res_lin_reg_cv}
lin_reg_res_1 %>% collect_metrics(summarize = FALSE)
```

There are some smaller differences between these folds, but overall those differences are not to big. Overall these metrics changed compared to the simple XGBoost model. The First model had a higher RMSE and a lower RSQ value.

Lets visualize these metrics into a plot. For this the predictions are plotted on the y-axis and the actual values on the x-axis. If a perfect model would have been created, all points would be on the diagonal line

```{r plot_lin_reg_res_1}

assess_lin_reg_res_1 <- collect_predictions(lin_reg_res_1)

assess_lin_reg_res_1 %>% 
  ggplot(aes(x = avg_score, y = .pred)) +   geom_point(alpha = .15) +
  geom_abline(col = "blue") + 
  coord_obs_pred() + 
  ylab("Predicted") +
  xlab("Actual Average Exam Score")



```

### Single Layer Perceptron

```{r res_neural_1}
set.seed(50)

neural_res_1 <-
  neural_workflow_1 %>%
  fit_resamples(
    resamples = cv_folds,
                control = control_resamples(save_pred = TRUE)
  )
neural_res_1 %>% collect_metrics(summarize = TRUE)
```

How are these metrics spreading through the cross_validation:

```{r res_neural_cv}
neural_res_1 %>% collect_metrics(summarize = FALSE)
```

There are some smaller differences between these folds, but overall those differences are not to big.

This model has the highest RSME score. The RSQ score is similar to the result of the XGBoost.

Lets visualize these metrics into a plot. For this the predictions are plotted on the y-axis and the actual values on the x-axis. If a perfect model would have been created, all points would be on the diagonal line

```{r plot_neural_res_1}

assess_neural_res_1 <- collect_predictions(neural_res_1)

assess_xgb_res_1 %>% 
  ggplot(aes(x = avg_score, y = .pred)) +   geom_point(alpha = .15) +
  geom_abline(col = "blue") + 
  coord_obs_pred() + 
  ylab("Predicted") +
  xlab("Actual Average Exam Score")

```

## Feature Selection

After the initial models were created and their results were visualized, lets have a look on the feature importance's. Currently all available features were used for the model creation. Therefore lets analyze the Feature Importance and therefore it might be better to remove some features.

```{r feature_importance_caret}

# ensure results are repeatable
set.seed(50)
# load the library
library(mlbench)
library(caret)
# load the dataset
data(train_data)
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(avg_score~ ., data=train_data, method="pls", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)

```

It can be seen that the other exam scores are having the biggest influence on the average score - which totally makes sense as these variables are the basis for this variable. This is exactly why they need to be removed from the modelling, even though they have a huge feature importance.

Beside these variables the most important features are the `lunch` and `test_preparation`variables. Also an important features seems to be the `ethnic_group` but only for one level (Group E). The other levels have only a very small influence on the result. Also the `gender` variable seems to be a bit important. The lowest feature importance is brought in by the `parent_education` as most of the levels have only a very small Importance. This result is matching the learning from the data visualization. In that chapter it was learned the most of the other levels have only a small impact on the average exam score.

**Forward Selection**

Lets make a simple model in which a Forward selection if performed. The result of this shall determine the variables which shall be used in all models.

```{r forward_selection}
# Set seed for reproducibility
set.seed(50)
# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 50)
# Train the model
step.model <- train(avg_score ~ gender + ethnic_group + parent_education + lunch + test_prep, data = train_data,
                    method = "leapForward", 
                    tuneGrid = data.frame(nvmax = 1:5), # only add 5 variables - we only have 5 in the data
                    trControl = train.control
                    )
step.model$results
```

It can be seen that we get the best scores for RMSE and R squared if all variables are added into the model. But the difference between 3, 4 and 5 variables is very small. Moreover there is no consideration of the adjusted R squared. R squared is increasing most times if additional variables are added into the model while the adjusted R squared is adding a penalty for this.

Nevertheless we will continue with the model which has the highest Rsquared and RMSE score. It needs to be added that both of these values are not that good.

Lets display the best tuning values (nvmax) selected by the train() function.

```{r best_tune}
step.model$bestTune
```

Result from Forward selection is to use all 5 variables for modelling.

Lets check in which order these variables are selected. This can be seen by the \* in the corresponding selection step.

```{r summary_feature_selection}
summary(step.model$finalModel)
```

Below is the result summed up. The rank of the most important features is:

1.  Lunch
2.  Test preparation
3.  ethnic_group
4.  parent_education
5.  gender

For better results of the model it would make sense to have a deeper analysis of the adjusted R squared score for the added variables. Moreover it would probably make sense to reduce the amount of levels in the variables. For example some ethnic groups have almost no difference between their performance on the average exam score.

## Model Tuning

This chapter will cover the possibilities of model tuning. For this the three created basis models will be tuned. In the first step the adaption of parameters is made manually. In the second step the tune package is used. For the model performance only the numeric results will be compared. No further analysis of the visualization of model results will be made. At the end the model with the best performance will be used to make a final fit on the training data.

As a comparison a model which only contains the mean will be created. This will be used as a simple comparison if the created model performs better than a simple guess of the mean for all exam scores.

### Boosted Tree

For the Boosted Tree the following hyperparameters shall be adapted:

-   Trees: Amount of trees in the ensemble - default 15

-   Tree_depth: Maximal depth of the tree (e.g number of splits) - default 6

-   min_n; minimum amount of data points for a further split - default 1

-   Learn_rate: Learn-rate at which the algorithm adapts from iteration to iteration - default 0.3

-   Loss_reduction: Minimum loss reduction - default 0.0

Source: [Boosted trees via xgboost](https://parsnip.tidymodels.org/reference/details_boost_tree_xgboost.html)

At first manually the hyper parameters are adapted and the result will be checked.

First of all the model will be specified:

```{r spec_xgboost_2}
xgb_spec_2 <- 
  boost_tree(
    trees = 30,
    tree_depth = 4,
    min_n = 8,
    learn_rate = 0.2,
    loss_reduction = 0.05) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression") 
```

Afterwards the new model will be added to the workflow with the created recipe

```{r workflow_xboost_2}

xgb_workflow_2 <-
  workflow() %>%
  add_recipe(exam_rec) %>%
  add_model(xgb_spec_2)
```

```{r res_xgboost_2}
set.seed(50)

xgb_res_2 <-
  xgb_workflow_2 %>%
  fit_resamples(
    resamples = cv_folds,
                control = control_resamples(save_pred = TRUE)
  )
xgb_res_2 %>% collect_metrics(summarize = TRUE)
```

The model performance improved by adapting the hyper parameters. The RMSE score could be reduced by 0.5. Now lets try it with the tune package.

The hyperparameter tuning is based on the following documentations:

-   https://www.tidymodels.org/start/tuning/

-   https://www.tidymodels.org/learn/work/tune-svm/

-   https://tune.tidymodels.org

First of all a new model specification is created. In this the hyperparameters are specified which shall be tuned.

Note: Due to to many different model variations, I have decided to reduce the grid to a maximum of three hyper parameters. Instead of additional hyper parameters, it makes more sense to test more levels in between.

```{r spec_xgboost_3}
xgb_spec_3 <- 
  boost_tree(
    trees = tune(),
    tree_depth = tune(),
    min_n = tune()) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression") 
```

Have a look on the tune spec:

```{r xgboost_tune_spec}
xgb_spec_3
```

The `tune()` is like a placeholder. After the tuning process single numeric values are selected for each of the hyperparameter.

In the next steps several models are trained using resampled data. Afterwards it is checked which model performs best. For this a grid of values is created:

```{r xgboost_grid}
xgboost_grid <- grid_regular(
  trees(),
  min_n(),
  tree_depth(),
  levels = 5
)
xgboost_grid
```

In total 125 different models with different hyperparameters were specified.

Everything is ready to tune. `tune_grid` can be used to fit models for all different hyperparameters.

Define the workflow with the model and recipe.

```{r workflow_xboost_3}

xgb_workflow_3 <-
  workflow() %>%
  add_recipe(exam_rec) %>%
  add_model(xgb_spec_3)
```

Calculate the results with tune_grid.

```{r res_xgboost_3}
set.seed(50)

xgb_res_3 <-
  xgb_workflow_3 %>%
  tune_grid(
    resamples = cv_folds,
    grid = xgboost_grid,
                control = control_resamples(save_pred = TRUE)
  )

```

The results of this can be filtered so that only the best 5 shall be displayed:

```{r xgboost_best_5_hyperpara}
xgb_res_3 %>%
  show_best("rmse")
```

Moreover a selection for the best tree can be made. With that the single set with the best hyperparameters is selected:

```{r xgboost_best}
xgboost_best <- xgb_res_3 %>%
  select_best("rmse")
xgboost_best
```

The hyper parameter optimization with the tune package increased the RMSE a lot compared to the initial model. It has also increased to the manual selected hyper parameters.

But it has to be said that I am skeptical regarding the selected best hyper parameters... Tree_depth of one and a min_n of two do not seem to be robust for new data...

### Linear regression

For the Linear Regression model the following hyper parameters shall be adapted:

-   Penalty: Amount of regularization - has no default and requires a single numeric value

-   Mixture: Proportion of lasso penalty - default 1.0

Source: [Linear regression via glmnet](https://parsnip.tidymodels.org/reference/details_linear_reg_glmnet.html)

At first manually the hyper parameters are adapted and the result will be checked.

First of all the model will be specified:

```{r spec_lin_reg_2}
lin_spec_2 <- 
  linear_reg(penalty = 0.3, mixture = 0.7) %>% 
  set_engine("glmnet") %>% 
  set_mode("regression") 
```

Afterwards the new model will be added to the workflow with the created recipe

```{r workflow_lin_reg_2}

lin_workflow_2 <-
  workflow() %>%
  add_recipe(exam_rec) %>%
  add_model(lin_spec_2)
```

```{r res_lin_reg_2}
set.seed(50)

lim_res_2 <-
  lin_workflow_2 %>%
  fit_resamples(
    resamples = cv_folds,
                control = control_resamples(save_pred = TRUE)
  )
lim_res_2 %>% collect_metrics(summarize = TRUE)
```

The model performance was not improved... Now lets try it with the tune package.

First of all a new model specification is created. In this the hyperparameters are specified which shall be tuned.

```{r spec_lin_reg_3}
lin_spec_3 <- 
  linear_reg(
    penalty = tune(),
    mixture = tune()) %>% 
  set_engine("glmnet") %>% 
  set_mode("regression") 
```

Have a look on the tune spec:

```{r line_reg_tune_spec}
lin_spec_3
```

The `tune()` is like a placeholder. After the tuning process single numeric values are selected for each of the hyper parameter.

In the next steps several models are trained using resampeled data. Afterwards it is checked which model performs best. For this a grid of values is created:

```{r line_reg_grid}
lin_grid <- grid_regular(
  penalty(),
  mixture(),
  levels = 5
)
lin_grid
```

In total 25 different models with different hyperparameters were specified.

Everything is ready to tune. `tune_grid` can be used to fit models for all different hyperparameters.

Define the workflow with the model and recipe.

```{r workflow_lin_reg_3}

lin_workflow_3 <-
  workflow() %>%
  add_recipe(exam_rec) %>%
  add_model(lin_spec_3)
```

Calculate the results with tune_grid.

```{r res_lin_reg_3}
set.seed(50)

lin_res_3 <-
  lin_workflow_3 %>%
  tune_grid(
    resamples = cv_folds,
    grid = lin_grid,
                control = control_resamples(save_pred = TRUE)
  )

```

The results of this can be filtered so that only the best 5 shall be displayed:

```{r lin_reg_best_5_hyperpara}
lin_res_3 %>%
  show_best("rmse")
```

Moreover a selection for the best tree can be made. With that the single set with the best hyper parameters is selected:

```{r lin_reg_best}
lin_best <- lin_res_3 %>%
  select_best("rmse")
lin_best
```

The hyper parameter optimization with the tune package increased the RMSE only by a minimum compared to the initial model. For the linear regression almost no improvement was possible.

The model performance is similar to the xgboost model which was tuned with the tune package.

### Single Layer Neural Network

For the Single Layer Neural Network the following hyperparameters shall be adapted:

-   hidden_units: #Hidden units - default 0

-   Penalty: Amount of regularization - default 0.0

-   epochs: Amount of epochs for training - default 100L

Source: [Multilayer perceptron via nnet](https://parsnip.tidymodels.org/reference/details_mlp_nnet.html)

At first manually the hyper parameters are adapted and the result will be checked.

First of all the model will be specified:

```{r spec_neural_2}
neural_spec_2 <- 
  mlp(
    hidden_units = 5,
    penalty = 0.1,
    epochs = 200) %>% 
  set_engine("nnet") %>% 
  set_mode("regression") 
```

Afterwards the new model will be added to the workflow with the created recipe

```{r workflow_neural_2}

neural_workflow_2 <-
  workflow() %>%
  add_recipe(exam_rec) %>%
  add_model(neural_spec_2)
```

```{r res_neural_2}
set.seed(50)

neural_res_2 <-
  neural_workflow_2 %>%
  fit_resamples(
    resamples = cv_folds,
                control = control_resamples(save_pred = TRUE)
  )
neural_res_2 %>% collect_metrics(summarize = TRUE)
```

The model performance improved a bit the hyper parameters. The RMSE score could be reduced by 0.3. Now lets try it with the tune package.

First of all a new model specification is created. In this the hyperparameters are specified which shall be tuned.

```{r spec_neural_3}
neural_spec_3 <- 
  mlp(
    hidden_units = tune(),
    penalty = tune(),
    epochs = tune()) %>% 
  set_engine("nnet") %>% 
  set_mode("regression") 
```

Have a look on the tune spec:

```{r neural_tune_spec}
neural_spec_3
```

The `tune()` is like a placeholder. After the tuning process single numeric values are selected for each of the hyper parameter.

In the next steps several models are trained using resampelled data. Afterwards it is checked which model performs best. For this a grid of values is created:

```{r neural_grid}
neural_grid <- grid_regular(
  hidden_units(),
  penalty(),
  epochs(),
  levels = 5
)
neural_grid
```

In total 125 different models with different hyperparameters were specified.

Everything is ready to tune. `tune_grid` can be used to fit models for all different hyperparameters.

Define the workflow with the model and recipe.

```{r workflow_neural_3}

neural_workflow_3 <-
  workflow() %>%
  add_recipe(exam_rec) %>%
  add_model(neural_spec_3)
```

Calculate the results with tune_grid.

```{r res_neural_3}
set.seed(50)

neural_res_3 <-
  neural_workflow_3 %>%
  tune_grid(
    resamples = cv_folds,
    grid = neural_grid,
                control = control_resamples(save_pred = TRUE)
  )

```

The results of this can be filtered so that only the best 5 shall be displayed:

```{r neural_best_5_hyperpara}
neural_res_3 %>%
  show_best("rmse")
```

Moreover a selection for the best tree can be made. With that the single set with the best hyperparameters is selected:

```{r neural_best}
neural_best <- neural_res_3 %>%
  select_best("rmse")
neural_best
```

The hyper parameter optimization with the tune package increased the RMSE a lot compared to the initial model. It has also increased to the manual selected hyper parameters.

The best models (based on lowest RMSE) are:

1.  RMSE: 12.10604 - Linear Regression
2.  RMSE: 12.10782 - XGBoost
3.  RMSE: 12.13196 - Neural Network

All models have very similar performance metrics and the differences are minor. But the RMSE score is still quite high. Probably the available (only categorical) variables are not that good for predicting the performance of the students in the exams. This was also seen in the previous chapters with the R squared score which was only slightly above 0.25.

In the next step the best model will be fitted a last time on the training data before the deployment phase can be started.

# Deployment

## Validate model

In this chapter the best model from the modeling will be fitted on all training data instead of only the split cross-validation data set. The final fitted model can be deployed and validated on the test data.

Add the model with the best results to the defined workflow:

```{r workflow_best_model}
workflow_best_model <-
  lin_workflow_3 %>%
  finalize_workflow(lin_best)
```

```{r model_last_fit}
last_fit_model <- workflow_best_model %>% 
  last_fit(data_split)
```

```{r}
last_fit_model %>%
    collect_metrics()
```

The performance of the model on test data is worse than it was on the training data. The RMSE metric increased from 12.1 to 12.77. Probably the model was overfitted a bit on the training data. But the RMSE score is still better than for certain created models in the Modelling phase (on training data).

## Deploy model

Sources for Model deployment:

-   [Youtube Posit: MLOps with vetiver in Python and R](https://www.youtube.com/watch?v=oFQANK13-k4)

-   [Posit Solutions: MLOps with vetiver and Posit](https://solutions.posit.co/gallery/model-management/vetiver/)

-   [Github: End-to-End machine Learning Pipeline](https://github.com/sol-eng/bike_predict)

-   [Posit: End-to_end Data Science with RStudio Connect](https://colorado.posit.co/rsc/bike-predict-r-dashboard/document.html)

-   https://cran.r-project.org/web/packages/vetiver/vetiver.pdf

-   [Plumber Documentation](https://www.rplumber.io/articles/introduction.html)

First of all a last fit of the created final model will be made. This is done as with the previous last fit the model is not compatible for deploying with vetiver. With this last fit on the training data the model fits to the requirements from vetiver

```{r last_fit_for_deployment}
last_fit_model_depl <- fit(workflow_best_model, data=train_data)

```

In the next step the model name and pin name gets specified.

Moreover the model is defined. For this the vetiver package is used.

```{r vetiver_model}
library(vetiver)
library(glue)
library(rsconnect)
library(pins)
model_name <- "Prediction_of_exam_score_model_r"
pin_name <- glue("daniel.steck/{model_name}")

v <- vetiver_model( #create vetiver model
  last_fit_model_depl,
  model_name,
  versioned = TRUE
)
v
```

After the model was defined the model is written to the pin in order to deploy it.

Sadly I could not find the information for Rsconnect in order to deploy it via an API in Rsconnect. Therefore the model will be written to a local and temporary board.

```{r pin_model}
model_board <- board_temp(versioned = TRUE) #authentifact information for rsconnect board not found, therefore no deployment on rsconnect
# Write the model to the board.
model_board %>%
 vetiver_pin_write(v)
```

Check all versions on the board:

```{r}
model_board %>% pin_versions(model_name)
```

## Serve model

Usage of plumber in order to deploy the model with vetiver.

If this code chunk is running it is possible to make predictions with the model in a separate window.

```{r}
library(plumber)
pr() %>%
  vetiver_api(v) %>%
  pr_run(port = 8000)
```

It is also possible to deploy the vetiver API to other infrastructures than Posit Connect (e.g. AWS or Azure). For this a Docker container helps a lot. This docker container will be created in the next code block.

```{r prepare_dockerfile}
#vetiver_prepare_docker(model_board, model_name)
```

Three files have been created:

-   Dockerfile

-   Plumber file serving the REST API

-   ventiver_renv_lock file to capture the model dependencies.

**Predict from the model endpoint:**

In case that the model is running in another process (e.g. remotely) it would be also possible to get predictions from this model in R. Below the endpoint definition is made as well as the request for a new student exam performance.

```{r define_endpoint}
endpoint <- vetiver_endpoint("http://127.0.0.1:8000/predict")
```

```{r predict_from_endpoint}
random_student <- tibble (
  gender = "female",
  ethnic_group = "group E",
  parent_education ="some college", 
  lunch = "standard",
  test_preparation = "completed"
)
#predict(endpoint, random_student)
```

Sadly this prediction is not working as I always get the error message, that the connection is refused...

## Monitor model

After the model was successfully deployed it is very important to monitor its statistical performance. The issue is that the decrease of model performance is not noticed as the model can continue returning predictions without errors. These quiet performance problems might occur when the statistical distribution of an input feature changes or when there is change in the relationship between input features and the outcome.

The silent failure can happen unnoticed if there is no monitoring for degradation.

The model monitoring did not work as I planned. I tried to solve it with the posit documentations and their examples. I assume that I am having issues as the model is not deployed to rsconnect and I only have it running locally as long as my code chunk on top is running.

Sources for the attempt of model monitoring:

-   [Posit Model metrics](https://colorado.posit.co/rsc/bike-predict-r-model-metrics/document.html)

-   [Vetiver compute metrics](https://rstudio.github.io/vetiver-r/reference/vetiver_compute_metrics.html)

-   [Vetiver Model monitoring](https://vetiver.rstudio.com/get-started/monitor.html)

## Shiny

Within this chapter the model and corresponding visualization shall be deployed with shiny and the target is to visualize everything in a dashboard.

First of all the connection has to be established. For this the data provided within the shiny account is used based on this documentation: [Getting started with shiny](https://shiny.rstudio.com/articles/shinyapps.html?_ga=2.28034556.1347876817.1682150371-373314606.1682150371)

```{r connection_rsconnect}
rsconnect::setAccountInfo(name='danielsteck', token='27FA72E141C5D1C0E638BBD54BB2421B', secret='kxHXXMfIFDDJDac/jIvWP7OOffeX8+eWZdDKl/Hu')
```

```{r load_shiny}
library(shiny)
```

Shiny demo application:

For creating shiny app it is required to build an separate server.R file as well as a ui.R file in which the data for the app are stored so that the app can be run. These files were created and are stored within this working directory.

**Test application**

```{r}
library(shiny)
runApp()
```
